{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnFSy/mSOvx3x2Eex9xX8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushs0911/Laptop-Price-Predictions/blob/main/Without%20output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "hSuIhP_Klqom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq5TNnNnDCri"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import colors\n",
        "cmap = colors.ListedColormap([\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"])\n",
        "palette= [\"#682F2F\",\"#F3AB60\"]\n",
        "pal = [\"#682F2F\",\"#B9C0C9\", \"#9F8A78\",\"#F3AB60\"]\n"
      ],
      "metadata": {
        "id": "lSYIGhnoiFBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling "
      ],
      "metadata": {
        "id": "26BxtHHVlzCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/laptop_data.csv\")"
      ],
      "metadata": {
        "id": "wsb0heOuDtbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "G1Ag17dmD1Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns "
      ],
      "metadata": {
        "id": "ef2na6NeD3oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropping the Unnamed column.**"
      ],
      "metadata": {
        "id": "3x5YjukzEHxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['Company', 'TypeName', 'Inches', 'ScreenResolution',\n",
        "       'Cpu', 'Ram', 'Memory', 'Gpu', 'OpSys', 'Weight', 'Price']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VY604ipxEBpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking null values"
      ],
      "metadata": {
        "id": "NWbnTrioEXN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "PehkviBBELPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicated rows. "
      ],
      "metadata": {
        "id": "KBN9oBgEEZ3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "enkalQWcEVGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "l7685OPSEfMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = df.select_dtypes(include = ['object']).columns \n",
        "numerical = df.select_dtypes(include = ['int32', 'int64', 'float32', 'float64']).columns \n",
        "\n",
        "categorical, numerical"
      ],
      "metadata": {
        "id": "evqfiCsLEgfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uniquevals(col):\n",
        "  print(f\"Details of the particular col {col} is : {df[col].unique()}\")\n",
        "\n",
        "def valuecounts(col):\n",
        "  print(f\"Valuecounts of the particular col {col} is : {df[col].value_counts()}\")"
      ],
      "metadata": {
        "id": "F-duQcZ4E54m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  uniquevals(col)\n",
        "  print('-'*75)"
      ],
      "metadata": {
        "id": "wWf3T4_nFsuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If we remove 'GB' from RAM, we can make it a integer value, same with Memory, Weight**"
      ],
      "metadata": {
        "id": "bWPa1kZFGrkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Ram'] = df['Ram'].str.replace('GB', '')\n",
        "df[\"Weight\"] = df['Weight'].str.replace('kg', '')\n",
        "\n",
        "#converting columns from string to Int \n",
        "df['Ram'] = df['Ram'].astype('int32')\n",
        "\n",
        "#converting to float \n",
        "df['Weight'] = df['Weight'].astype('float32')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "UD8MKXthF0Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "FnIjbVDhHfe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis "
      ],
      "metadata": {
        "id": "ONd-KLliSih6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#viewing the distribution of the price column\n",
        "sn.distplot(df.Price, color = 'red')"
      ],
      "metadata": {
        "id": "2vUQFImHSiWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little bit Left Skewed Gaussian Distribution. "
      ],
      "metadata": {
        "id": "Tj03s1LJS9Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the countplots for categorical variables \n",
        "\n",
        "def drawplot(col):\n",
        "  plt.figure(figsize = (10,7))\n",
        "  sn.countplot(data = df, x = col, palette= pal)\n",
        "  plt.xticks(rotation = 'vertical')"
      ],
      "metadata": {
        "id": "Ok-mMPK-SeMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view = ['Company', 'TypeName', 'Ram', 'OpSys']\n",
        "for col in view:\n",
        "  drawplot(col)"
      ],
      "metadata": {
        "id": "voEUvVvfTg76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average price for each of the laptop brands \n",
        "# this will give us the insight that as per compnay, how the price of the laptop vary \n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.barplot(x = df['Company'], y = df['Price'], palette= pal)\n",
        "plt.xticks(rotation = 'vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AwnZsa86Udsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# various types of laptops \n",
        "\n",
        "sn.countplot(data = df, x = 'TypeName', palette= pal)\n",
        "plt.xticks(rotation = 'vertical')"
      ],
      "metadata": {
        "id": "7h-h7pgsW2fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# laptop type and variation about the price \n",
        "\n",
        "sn.barplot(x = df['TypeName'],y = df['Price'], palette= pal)\n",
        "plt.xticks(rotation = 'vertical')\n"
      ],
      "metadata": {
        "id": "rQQyIolyXWAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook which is higest selling type, gives the minimum price range. Affordability makes it highest selling product. "
      ],
      "metadata": {
        "id": "WvJOs6TIXw4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# variations of incjes towards the price \n",
        "\n",
        "sn.scatterplot(x = df['Inches'], y = df['Price'])"
      ],
      "metadata": {
        "id": "CSYER2wFXrBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For the `Screen Resolution` column we have many types of Screen Resolutions out there as shown `Touch Screen` and `Normal` and `IPS Panel` are the 3 parts on basis of which we can segregate the things**"
      ],
      "metadata": {
        "id": "iKkORfLdYT1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['ScreenResolution'].value_counts()"
      ],
      "metadata": {
        "id": "OrkyHaxfYIoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['TouchScreen'] = df['ScreenResolution'].apply(lambda element: 1 \n",
        "                                                 if 'Touchscreen' in element \n",
        "                                                 else 0)"
      ],
      "metadata": {
        "id": "KI4UOe69YmV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.countplot(df, x = 'TouchScreen', palette= palette)"
      ],
      "metadata": {
        "id": "GYmBwvrdZA-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# touch screen on comparison with price of laptop \n",
        "\n",
        "sn.barplot(x = df.TouchScreen, y = df.Price, palette= palette)\n",
        "plt.xticks(rotation = 'vertical')"
      ],
      "metadata": {
        "id": "n5QD4j45ZC23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new col names IPS, does the laptop have IPS facility or not \n",
        "df ['IPS'] = df['ScreenResolution'].apply(\n",
        "    lambda element : 1 if 'IPS' in element else 0\n",
        ")"
      ],
      "metadata": {
        "id": "ZAujyVJea_7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample()"
      ],
      "metadata": {
        "id": "kO_PW2MMb7CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.countplot(df, x = 'IPS', palette= palette)"
      ],
      "metadata": {
        "id": "EdyYHZwqbmTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.barplot(x = df.IPS, y = df.Price, palette= palette)\n",
        "plt.xticks(rotation = 'vertical')"
      ],
      "metadata": {
        "id": "JtSYsuSlbswy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting the X resolution and Y Resolution**"
      ],
      "metadata": {
        "id": "gWGYN2k4cbg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we will split the text at the \"x\" letter and separate the 2 parts \n",
        "\n",
        "splitdf = df['ScreenResolution'].str.split('x', n =1, expand = True)\n",
        "splitdf.head()"
      ],
      "metadata": {
        "id": "_jBScZVvcVV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['X_res'] = splitdf[0]\n",
        "df['Y_res'] = splitdf[1]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "H7ZleEDodRtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to extract number from `X_res`, we need to extract the digits from it. \n",
        "\n",
        "Using `regex` to exactly get the numbers which we are looking for. \n",
        "- replacing all \",\" with \"\" and then find numbers \n",
        "- `\\d+\\.?\\d+` means the integer number and `\\.?` all the numbers which come after a number and `\\d+` the string must end with number. "
      ],
      "metadata": {
        "id": "BEgNsCfmgKyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['X_res'] = df['X_res'].str.replace(',', '').str.findall(r'(\\d+\\.?\\d+)').apply(lambda x:x[0])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ES9U6OsEd5jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['X_res'] = df['X_res'].astype('int')\n",
        "df['Y_res'] = df['Y_res'].astype('int')"
      ],
      "metadata": {
        "id": "Al00geq_hQk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df.corr(), annot = True, cmap = cmap)"
      ],
      "metadata": {
        "id": "9teK10zWh0DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()['Price']"
      ],
      "metadata": {
        "id": "sOedJDNZiUNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "above results show that `X_res` and `Y_res` are positively correlated, so we can combine them with `Inches` which is giving less collinearity. <br>\n",
        "We can create a new column named `PPI(pixles per inch)`\n",
        "\n",
        "$$\n",
        "PPI(pixels per inch) = \\frac{\\sqrt{X_resolution^2+Y_resolution^2}}{inches}\n",
        "$$"
      ],
      "metadata": {
        "id": "q_zf0T4wrVj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['PPI'] = (((df['X_res']**2+df['Y_res']**2))**0.5/df['Inches']).astype('float')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_dbodC2YrFwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()['Price']"
      ],
      "metadata": {
        "id": "6yWeYmL0tTib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it can be seen from the correlation data that the `PPI` is having good correlaiton, so we will be using that, as that is combination of 3 features and gives collective results of 3 columns, so we will drop `Inches`, `X_res`, `Y_res` as well"
      ],
      "metadata": {
        "id": "2M3Cyo_Ctnc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns = ['ScreenResolution', 'Inches', 'X_res','Y_res'], inplace = True)"
      ],
      "metadata": {
        "id": "GB8T9VaHtihh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "bJMNoMaZuK0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processing `Cpu` column.**"
      ],
      "metadata": {
        "id": "85I22AyGvy1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cpu'].value_counts()"
      ],
      "metadata": {
        "id": "kMj0SyjQuauI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most common processors are by Intel, so we will be clustering their processors into different categories like `i5, i7, other`, now other means the processors of intel which do not have i3, i5 or i7 attached to it, they're completely diffrent so that's the reason I will clutter them into `other` and other category is `AMD` which is a diffrent category in whole. "
      ],
      "metadata": {
        "id": "LB5OzVzHwFbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Intel Core i5 7200U 2.5GHz\"\n",
        "' '.join(text.split()[:3])"
      ],
      "metadata": {
        "id": "aDEcFxBZxZ-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['CPU_name'] = df['Cpu'].apply(lambda text : \" \".join(text.split()[:3]))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Un1_PpMQv6ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we get any of the intel `i3, i5 or i7` versions we will return them as it is, but if we get any other processor. <br>\n",
        "We will first check whether is that a variant of intel? or not. <br>\n",
        "If yes, then we will tag it as 'Other Intel Processor' else we will say it as 'AMD Processor'. "
      ],
      "metadata": {
        "id": "wF5FtnNdzGoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processortype(text):\n",
        "  if text =='Intel Core i7' or text == 'Intel Core i5' or text == 'Intel Core i3':\n",
        "    return text \n",
        "  \n",
        "  else :\n",
        "    if text.split()[0] == 'Intel':\n",
        "      return \"Other Intel Processor\"\n",
        "    else: \n",
        "      return 'AMD Processor'"
      ],
      "metadata": {
        "id": "rXdAkjvJwvzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['CPU_name'] = df['CPU_name'].apply(lambda text:processortype(text))"
      ],
      "metadata": {
        "id": "vKdzLr480FqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['CPU_name'].value_counts()"
      ],
      "metadata": {
        "id": "tJC8DSnJ0VMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#price vs processor variation \n",
        "sn.barplot(x = df['CPU_name'], y = df['Price'], palette = palette)\n",
        "plt.xticks(rotation = 'vertical')"
      ],
      "metadata": {
        "id": "gCbdWPds0Whg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##dropping the CPU column \n",
        "df.drop(columns = ['Cpu'], inplace = True)\n",
        "df.head(1)"
      ],
      "metadata": {
        "id": "8zLrKkGk2QxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis on the RAM Column**"
      ],
      "metadata": {
        "id": "1QLHX-Qj7J9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sn.countplot(df, x = 'Ram', palette = palette)"
      ],
      "metadata": {
        "id": "mw5nCsD-61RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# price and RAM relation \n",
        "\n",
        "sn.barplot(x = df.Ram, y = df.Price, palette = palette)"
      ],
      "metadata": {
        "id": "-9YI5roi7f_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Memory Column**<br>\n"
      ],
      "metadata": {
        "id": "5j5OmhlC8bz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Memory'].value_counts()"
      ],
      "metadata": {
        "id": "vohvIbcD7v3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 most common variants observed : HHD, SSD, Flash, Hybrid(SSD + HHD). "
      ],
      "metadata": {
        "id": "d0JNE-K386OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing decimal space --> 1.0 TB to 1 TBz\n",
        "#some columns have these floats\n",
        "df['Memory'] = df['Memory'].astype(str).replace('\\.0', '', regex = True)"
      ],
      "metadata": {
        "id": "Fe3lQO7D8h6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing GB word with \" \"\n",
        "df['Memory'] = df['Memory'].str.replace('GB', '')"
      ],
      "metadata": {
        "id": "d8yDoANQ9jmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace TB word with 000\n",
        "df['Memory'] = df['Memory'].str.replace('TB', '000')"
      ],
      "metadata": {
        "id": "2tSSG83e9v3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the word across '+' character \n",
        "newdf = df['Memory'].str.split('+', n = 1, expand = True)"
      ],
      "metadata": {
        "id": "P3dsMorL95jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdf"
      ],
      "metadata": {
        "id": "tg5dCH4W-HqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['first'] = newdf[0]\n",
        "df['first'] = df['first'].str.strip()\n",
        "df.head(1)"
      ],
      "metadata": {
        "id": "ZLgNhNvs-I_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def applychanges(value):\n",
        "    \n",
        "    df['Layer1'+value] = df['first'].apply(lambda x:1 if value in x else 0)\n",
        "    \n",
        "    \n",
        "listtoapply = ['HDD','SSD','Hybrid','FlashStorage']    \n",
        "for value in listtoapply:\n",
        "    applychanges(value)\n",
        "    \n",
        "    \n",
        "df.head()"
      ],
      "metadata": {
        "id": "wtVp3LicEPBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all the characters just keep the numbers\n",
        "\n",
        "df['first'] = df['first'].str.replace(r'\\D','')\n",
        "df['first'].value_counts()"
      ],
      "metadata": {
        "id": "1YMq6VPAEWtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Second\"] = newdf[1]"
      ],
      "metadata": {
        "id": "2PPRi984Elzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def applychanges1(value):\n",
        "    \n",
        "    df['Layer2'+value] = df['Second'].apply(lambda x:1 if value in x else 0)\n",
        "    \n",
        "    \n",
        "listtoapply1 = ['HDD','SSD','Hybrid','FlashStorage']\n",
        "df['Second'] = df['Second'].fillna(\"0\")\n",
        "for value in listtoapply1:\n",
        "    applychanges1(value)\n",
        "    \n",
        "\n",
        "# remove all the characters just keep the numbers\n",
        "\n",
        "df['Second'] = df['Second'].str.replace(r'\\D','')\n",
        "df['Second'].value_counts()"
      ],
      "metadata": {
        "id": "C5kckyC4Fbvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['first'] = df['first'].astype('int')\n",
        "df['Second'] = df['Second'].astype('int')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5OVwoRhrE44d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiplying the elements and storing the result in subsequent columns\n",
        "\n",
        "\n",
        "df[\"HDD\"]=(df[\"first\"]*df[\"Layer1HDD\"]+df[\"Second\"]*df[\"Layer2HDD\"])\n",
        "df[\"SSD\"]=(df[\"first\"]*df[\"Layer1SSD\"]+df[\"Second\"]*df[\"Layer2SSD\"])\n",
        "df[\"Hybrid\"]=(df[\"first\"]*df[\"Layer1Hybrid\"]+df[\"Second\"]*df[\"Layer2Hybrid\"])\n",
        "df[\"Flash_Storage\"]=(df[\"first\"]*df[\"Layer1FlashStorage\"]+df[\"Second\"]*df[\"Layer2FlashStorage\"])\n",
        "\n",
        "\n",
        "## dropping of uncessary columns\n",
        "\n",
        "df.drop(columns=['first', 'Second', 'Layer1HDD', 'Layer1SSD', 'Layer1Hybrid',\n",
        "       'Layer1FlashStorage', 'Layer2HDD', 'Layer2SSD', 'Layer2Hybrid',\n",
        "       'Layer2FlashStorage'],inplace=True)"
      ],
      "metadata": {
        "id": "aXe3qSCSFELb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "id": "PCAxpkx2GHw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns = [\"Memory\"], inplace = True)"
      ],
      "metadata": {
        "id": "00QJX7lkGL4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()['Price']"
      ],
      "metadata": {
        "id": "_ktn-7hbGVeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the correlation we observe that `Hybrid` and `Flash Storage` are almost negligible, we can simply drop them off. <br>\n",
        "`HOD` and `SSD` are giving good correlation, `HOD` have negative relation with Price. "
      ],
      "metadata": {
        "id": "Rc66QZwMG9I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns "
      ],
      "metadata": {
        "id": "LTLQtCSoGYva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns = ['Hybrid', 'Flash_Storage'], inplace = True)"
      ],
      "metadata": {
        "id": "sFiFdD_BMb7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis on GPU"
      ],
      "metadata": {
        "id": "XjesPwf8Mq-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gpu'].value_counts()"
      ],
      "metadata": {
        "id": "cymbkkuZMj7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gpu brand'] = df['Gpu'].apply(lambda x:x.split()[0])"
      ],
      "metadata": {
        "id": "SNHZfT3qMtLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.countplot(df, x = 'Gpu brand',palette=palette)"
      ],
      "metadata": {
        "id": "zD3wbVTlMz1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the 'ARM' tuple \n",
        "\n",
        "df = df[df['Gpu brand']!= 'ARM']"
      ],
      "metadata": {
        "id": "RtVx_sGBM2hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.countplot(df, x = 'Gpu brand',palette=palette)"
      ],
      "metadata": {
        "id": "dt9KZqmlNLxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.barplot(x = df['Gpu brand'], y = df.Price, estimator = np.median, palette = palette)"
      ],
      "metadata": {
        "id": "gZSBYbLSNTFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns = ['Gpu'])\n",
        "df.head(1)"
      ],
      "metadata": {
        "id": "RtM3EQ3MNnmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operating System Analysis"
      ],
      "metadata": {
        "id": "BbWz6UfSN738"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['OpSys'].value_counts()"
      ],
      "metadata": {
        "id": "g4eaHPVONwzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.barplot( x = df['OpSys'], y = df['Price'], palette = palette)\n",
        "plt.xticks(rotation = 'vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ZY3RIuYOEJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Clubbing Windows 10, windows 7, windows 7S --> Windows \n",
        "- club macOS, macOS x ---> mac \n",
        "- else return others. "
      ],
      "metadata": {
        "id": "SykfLuZcPhcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setcategory(text):\n",
        "    if text=='Windows 10' or text=='Windows 7' or text=='Windows 10 S':\n",
        "        return 'Windows'\n",
        "    elif text=='Mac OS X' or text=='macOS':\n",
        "        return 'Mac'\n",
        "    else:\n",
        "        return 'Other'\n",
        "    \n",
        "    \n",
        "df['OpSys'] = df['OpSys'].apply(lambda x:setcategory(x))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FJ1ficExOKLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.countplot(df, x = 'OpSys', palette=palette)\n"
      ],
      "metadata": {
        "id": "HaA0HoA7QJeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.barplot(x = df['OpSys'],y = df['Price'], palette = palette)\n",
        "plt.xticks(rotation = 'vertical')"
      ],
      "metadata": {
        "id": "Hj7XhVb5QUzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight Analysis**"
      ],
      "metadata": {
        "id": "2uf2Vwq0QeoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sn.distplot(df['Weight'])"
      ],
      "metadata": {
        "id": "ZT9lyTEiQZjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.scatterplot(df, x= 'Weight' , y = 'Price')"
      ],
      "metadata": {
        "id": "q3rQA0mxQjak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price Analysis "
      ],
      "metadata": {
        "id": "x8IASUvFTREA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sn.distplot(df['Price'])"
      ],
      "metadata": {
        "id": "snFqBQfoS162"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Left Skewed gaussian distribution, we can try applying log to convert it into Uniform Gaussian distribution. "
      ],
      "metadata": {
        "id": "jXUSyFwhUupv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sn.distplot(np.log(df['Price']))"
      ],
      "metadata": {
        "id": "HnHJhNclTkt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()['Price']"
      ],
      "metadata": {
        "id": "c3tzCkVJTm-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sn.heatmap(df.corr(),annot=True,cmap=cmap)"
      ],
      "metadata": {
        "id": "PAdJs2ClTpit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Building "
      ],
      "metadata": {
        "id": "nz4YPoOYT8i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = np.log(df['Price'])\n",
        "train = df.drop(['Price'], axis = 1)"
      ],
      "metadata": {
        "id": "SN4zd6iGTsP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn import tree"
      ],
      "metadata": {
        "id": "95vpSgp0WhGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train,test,\n",
        "                                                   test_size=0.15,random_state=2)\n",
        "\n",
        "X_train.shape,X_test.shape"
      ],
      "metadata": {
        "id": "VDvAq-GYWg7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Column Transfer` used to build model pipelines, for this we have to get the index numbers of columns which are having categorical variables. "
      ],
      "metadata": {
        "id": "ro_B0ZkqW2qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapper = {i:value for i, value in enumerate(X_train.columns)}\n",
        "mapper"
      ],
      "metadata": {
        "id": "MBFfXsYJWliT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression "
      ],
      "metadata": {
        "id": "FESyekauXa8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will Apply one hot encoding on the columns with this indices --> [0,3,8,11], the remainder we keep as passthrough,i.e., no other col must get effected except the ones undergoing the transformation. "
      ],
      "metadata": {
        "id": "hoPKZWd9Xv_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use of scikit-learn's `ColumnTransformer`, `LinearRegression`, and `Pipeline` classes to create a ML pipeline for data preprocessing and regression.\n",
        "\n",
        "Step 1: ColumnTransformer\n",
        "The `ColumnTransformer` class : apply different preprocessing steps to different columns of the input data. \n",
        "- The `transformers` argument specifies a list of tuples, where each tuple contains a name for the transformation ('col_tnf') and the transformer to be applied (`OneHotEncoder`).\n",
        "- The `sparse=False` argument specifies that the encoded output should be returned as a dense array rather than a sparse matrix.\n",
        "- The `drop='first'` argument instructs the `OneHotEncoder` to drop the first category in each encoded feature, which avoids multicollinearity issues.\n",
        "\n",
        "The `remainder='passthrough'` argument specifies that any columns not explicitly specified in the transformers list should be passed through without any transformations.\n",
        "\n",
        "\n",
        "Pipeline:\n",
        "The `Pipeline` class is used to chain multiple steps together into a single object that can be used as a single estimator. It sequentially applies a list of transformers and ends with an estimator.  \n",
        "\n",
        "- The first step in the pipeline is `step1`, which represents the `ColumnTransformer` defined earlier.\n",
        "- The second step is `step2`, which represents the `LinearRegression` model."
      ],
      "metadata": {
        "id": "w_670FsL0P_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,1,3,8,11])\n",
        "],remainder='passthrough')\n",
        "\n",
        "step2 = LinearRegression()\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "print('R2 score',metrics.r2_score(y_test,y_pred))\n",
        "print('MAE',metrics.mean_absolute_error(y_test,y_pred)) "
      ],
      "metadata": {
        "id": "WgDSM0HLXM7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regression\n",
        "Ridge regression is a regularization technique used in linear regression to mitigate the problem of multicollinearity and overfitting. It adds a penalty term to the ordinary least squares (OLS) objective function to control the complexity of the model and reduce the impact of correlated features.\n",
        "\n",
        "In ordinary least squares, the goal is to minimize the sum of squared differences between the predicted values and the actual target values. However, when the input features are highly correlated, the coefficient estimates can become sensitive to small changes in the input data. This leads to instability and overfitting.\n",
        "\n",
        "Ridge regression addresses this issue by introducing a regularization term that penalizes large coefficient values.  \n",
        "\n",
        "The addition of the regularization term encourages the model to find coefficient values closer to zero, effectively reducing their impact on the predictions. The larger the alpha value, the stronger the penalty and the more the coefficients are shrunk.\n",
        "\n",
        "By shrinking the coefficients, ridge regression helps to mitigate multicollinearity by reducing the impact of highly correlated features. This leads to a more stable and robust model, with less sensitivity to minor changes in the input data. However, it is important to note that ridge regression does not perform feature selection or eliminate irrelevant features entirely. Instead, it reduces their impact.\n",
        "\n",
        "To apply ridge regression, the alpha value needs to be chosen. This is typically done through cross-validation, where different alpha values are tested, and the one that provides the best performance on unseen data is selected."
      ],
      "metadata": {
        "id": "hACd9JifinbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,1,3,8,11])\n",
        "],remainder='passthrough')\n",
        "\n",
        "step2 = Ridge(alpha=10)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "print('R2 score',metrics.r2_score(y_test,y_pred))\n",
        "print('MAE',metrics.mean_absolute_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "zvZqBX8rinCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso "
      ],
      "metadata": {
        "id": "HpZKqkrqjMTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One difference, in case of Lasso Regression the Shrinkage Term(Penalty Term) forces some of the model coefficients to become exactly 0 thereby removing the entire feature from the model (given that the λ value is large enough). This gives a whole new application of **Lasso Regression — Feature Selection**. This is not possible in case of Ridge Regression.\n",
        "\n",
        "\n",
        "By promoting sparsity, lasso can effectively identify and select the most relevant features, removing irrelevant or redundant features from the model. This can lead to improved interpretability and more efficient models.\n",
        "\n",
        "As with ridge regression, the alpha value needs to be chosen to balance the level of regularization. Cross-validation is commonly used to select the optimal alpha value by evaluating the model's performance on unseen data.\n",
        "\n",
        "In scikit-learn, you can use the Lasso class to perform lasso regression. It provides methods for fitting the model, making predictions, and accessing the coefficient values. Additionally, scikit-learn also provides the ElasticNet class, which combines the penalties of both ridge regression and lasso, allowing for a mix of both regularization techniques."
      ],
      "metadata": {
        "id": "fMc38ikO9umv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,1,3,8,11])\n",
        "],remainder='passthrough')\n",
        "\n",
        "step2 = Lasso(alpha=0.001)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "print('R2 score',metrics.r2_score(y_test,y_pred))\n",
        "print('MAE',metrics.mean_absolute_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "b9SCYzcqZKAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "P0ZkvRa4jSUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,1,3,8,11])\n",
        "],remainder='passthrough')\n",
        "\n",
        "step2 = DecisionTreeRegressor(max_depth=8)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "print('R2 score',metrics.r2_score(y_test,y_pred))\n",
        "print('MAE',metrics.mean_absolute_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "R4--WqIkjQC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "qRHuRm4Ulyg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest regression is an ensemble learning method that combines multiple decision trees to make predictions. It is a powerful and popular technique for regression tasks that provides improved accuracy and robustness compared to individual decision trees.\n",
        "\n",
        "Here's how Random Forest regression works:\n",
        "\n",
        "1. Data Sampling:\n",
        "   Random Forest uses a technique called Bootstrap Aggregating, or \"bagging,\" to create multiple subsets of the original training data. Each subset, called a \"bootstrap sample,\" is created by randomly selecting data points from the original training set with replacement. These subsets are used to train individual decision trees.\n",
        "\n",
        "2. Building Decision Trees:\n",
        "   For each bootstrap sample, a decision tree is constructed using the following steps:\n",
        "   - Feature Selection: At each node of the decision tree, a random subset of features is considered for splitting. This random subset of features helps introduce diversity among the trees and reduce correlation.\n",
        "   - Splitting: The decision tree is built by recursively selecting the best feature and split point based on a chosen criterion (such as Gini impurity or information gain).\n",
        "   - Tree Growth: The tree continues to grow until a specified stopping criterion is met, such as reaching a maximum depth or minimum number of samples in a leaf node.\n",
        "\n",
        "3. Aggregating Predictions:\n",
        "   Once all the individual decision trees are built, predictions are made by aggregating the predictions of each tree. For regression, the predictions from each tree are averaged to obtain the final prediction.\n",
        "\n",
        "The main advantages of Random Forest regression include:\n",
        "- Reduction of overfitting: Random Forest helps mitigate overfitting by using multiple trees with different subsets of data and features. This helps to capture a more generalized pattern from the data.\n",
        "- Robustness: Random Forest is less sensitive to outliers and noisy data compared to a single decision tree.\n",
        "- Feature Importance: Random Forest provides a measure of feature importance based on how much the mean squared error (MSE) is reduced by each feature across all the trees. This information can be useful for feature selection.\n",
        "\n",
        "In scikit-learn, you can use the `RandomForestRegressor` class to implement Random Forest regression. It provides methods for fitting the model, making predictions, and accessing feature importances."
      ],
      "metadata": {
        "id": "HVyigCeNDjKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,1,3,8,11])\n",
        "],remainder='passthrough')\n",
        "\n",
        "step2 = RandomForestRegressor(n_estimators=100,\n",
        "                              random_state=3,\n",
        "                              max_samples=0.5,\n",
        "                              max_features=0.75,\n",
        "                              max_depth=15)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "print('R2 score',metrics.r2_score(y_test,y_pred))\n",
        "print('MAE',metrics.mean_absolute_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "netFUsaojdVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(df, open('df.pkl', 'wb'))\n",
        "pickle.dump(pipe, open('pipe.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "djcEJZDoPW1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('traineddata.csv', index = None)"
      ],
      "metadata": {
        "id": "1cXAGU1vQw9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexlist = [0,1,3,8,11]\n",
        "transformlist = []\n",
        "for key, value in mapper.items():\n",
        "  if key in indexlist:\n",
        "    transformlist.append(value)"
      ],
      "metadata": {
        "id": "_wp9XTWLSRs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformlist "
      ],
      "metadata": {
        "id": "N2dblyciTnkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.get_dummies(train, columns = transformlist, drop_first = True)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "IhMEshscTprF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train,test,\n",
        "                                                   test_size=0.15,random_state=2)\n",
        "\n",
        "X_train.shape,X_test.shape"
      ],
      "metadata": {
        "id": "KrPw-NtBT0fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = DecisionTreeRegressor(random_state=0)\n",
        "reg.fit(X_train,y_train)\n",
        "plt.figure(figsize=(16,9))\n",
        "tree.plot_tree(reg,filled=True,feature_names=train.columns)"
      ],
      "metadata": {
        "id": "PbX4cjPJT6n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have to optimize it. "
      ],
      "metadata": {
        "id": "Cp_9hNDBUbrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = reg.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas "
      ],
      "metadata": {
        "id": "xWeF1HdFUA96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphalist = []\n",
        "for alpha in ccp_alphas:\n",
        "  reg = DecisionTreeRegressor(random_state = 0, ccp_alpha = alpha)\n",
        "  reg.fit(X_train, y_train)\n",
        "  alphalist.append(reg)"
      ],
      "metadata": {
        "id": "pU0j5a8tUnhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_score = [reg.score(X_train, y_train) for reg in alphalist]\n",
        "test_score = [reg.score(X_test, y_test) for reg in alphalist]\n",
        "\n",
        "plt.xlabel('ccp alpha')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.plot(ccp_alphas, train_score, marker = 'o', \n",
        "         label = 'training', color = 'magenta')\n",
        "plt.plot(ccp_alphas, test_score, marker = '+', \n",
        "         label = 'testing', color = 'red', drawstyle = 'steps-post')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2b6qR1YVVvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**possible values of alpha can lie between `[0.0025-->0.0075]`**"
      ],
      "metadata": {
        "id": "jEiFerkBXbrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg = DecisionTreeRegressor(random_state=0,ccp_alpha=0.0085)\n",
        "reg.fit(X_train,y_train)\n",
        "plt.figure(figsize=(16,9))\n",
        "tree.plot_tree(reg,filled=True,feature_names=train.columns)"
      ],
      "metadata": {
        "id": "eDxnMLjyVlff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Going till optimal deep. \n",
        "- short and simple tree \n",
        "- MSE also falling down. "
      ],
      "metadata": {
        "id": "BGlQpCdNXore"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Lwp54GBcSgcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params=  {\n",
        "    \n",
        "    'RandomForest':{\n",
        "        'model' : RandomForestRegressor(),\n",
        "        'params':{\n",
        "            'n_estimators':[int(x) for x in np.linspace(100,1200,10)],\n",
        "            'criterion':['squared_error'],\n",
        "            'max_depth':[int(x) for x in np.linspace(1,30,5)],\n",
        "            'max_features':['auto','sqrt','log2'],\n",
        "            'ccp_alpha':[x for x in np.linspace(0.0025,0.0125,5)],\n",
        "            'min_samples_split':[2,5,10,14],\n",
        "            'min_samples_leaf':[2,5,10,14],\n",
        "        }\n",
        "    },\n",
        "    'Decision Tree':{\n",
        "        'model':DecisionTreeRegressor(),\n",
        "        'params':{\n",
        "            'criterion':['squared_error'],\n",
        "            'max_depth':[int(x) for x in np.linspace(1,30,5)],\n",
        "            'max_features':['auto','sqrt','log2'],\n",
        "            'ccp_alpha':[x for x in np.linspace(0.0025,0.0125,5)],\n",
        "            'min_samples_split':[2,5,10,14],\n",
        "            'min_samples_leaf':[2,5,10,14],\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "m7IU0bOZXkbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "\n",
        "for modelname, mp in params.items():\n",
        "  clf = RandomizedSearchCV(mp['model'],\n",
        "                           param_distributions = mp['params'], cv = 5,\n",
        "                           n_iter = 10, scoring = 'neg_mean_squared_error', verbose =2)\n",
        "  clf.fit(X_train, y_train)\n",
        "  scores.append({\n",
        "      'model_name' : modelname,\n",
        "      'best_score' : clf.best_score_,\n",
        "      'best_estimator' : clf.best_estimator_,\n",
        "  })\n"
      ],
      "metadata": {
        "id": "FvqIxmJ7YykT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df = pd.DataFrame(scores, columns = ['model_name', 'best_score', 'best_estimator'])"
      ],
      "metadata": {
        "id": "YLDVTMbWZy0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df"
      ],
      "metadata": {
        "id": "vtL-kMbSbQ3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "sBjFyLeobTfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(ccp_alpha=0.0025, max_depth=22, min_samples_leaf=14,\n",
        "                        min_samples_split=5, n_estimators=1200)\n",
        "\n",
        "rf.fit(X_train,y_train)\n",
        "ypred = rf.predict(X_test)\n",
        "print(metrics.r2_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "fM46vErCbmKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction on whole dataset "
      ],
      "metadata": {
        "id": "qv66vamGc2JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = []\n",
        "testtrain = np.array(train)\n",
        "for i in range(len(testtrain)):\n",
        "    predicted.append(rf.predict([testtrain[i]]))\n",
        "    \n",
        "predicted"
      ],
      "metadata": {
        "id": "1z5kcBVRcsP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as we transformed our price variable to np.log\n",
        "# we have to retranform it from np.log-->np.exp inorder to get the result\n",
        "\n",
        "ans = [np.exp(predicted[i][0]) for i in range(len(predicted))]"
      ],
      "metadata": {
        "id": "6DHODnwedMhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Predicted Price'] = np.array(ans)\n",
        "df"
      ],
      "metadata": {
        "id": "YiEnolNgePnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.distplot(df['Price'], hist = False, color = 'orange', label = 'Actual')\n",
        "sn.distplot(df['Predicted Price'], hist = False, color = 'blue', label = 'Prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BYjNblzQeYfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is not a good result. "
      ],
      "metadata": {
        "id": "h9OkssUxfL14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Regressor version_2"
      ],
      "metadata": {
        "id": "3MF7a3fQfZE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf1 = RandomForestRegressor(n_estimators=100,\n",
        "                              random_state=3,\n",
        "                              max_samples=0.5,\n",
        "                              max_features=0.75,\n",
        "                              max_depth=15)\n",
        "\n",
        "rf1.fit(X_train,y_train)\n",
        "print(f'R2 score : {metrics.r2_score(y_test,rf1.predict(X_test))}')"
      ],
      "metadata": {
        "id": "PKtWPUqoewbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = []\n",
        "testtrain = np.array(train)\n",
        "for i in range(len(testtrain)):\n",
        "    predicted.append(rf1.predict([testtrain[i]]))\n",
        "    \n",
        "predicted"
      ],
      "metadata": {
        "id": "CnMq2sWmfaur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = [np.exp(predicted[i][0]) for i in range(len(predicted))]"
      ],
      "metadata": {
        "id": "UcU-IBY1fcNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.copy()\n",
        "data['Predicted Price'] = np.array(ans)\n",
        "data"
      ],
      "metadata": {
        "id": "fOi3rTlifeKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.distplot(data['Price'],hist=False,color='orange',label='Actual')\n",
        "sn.distplot(data['Predicted Price'],hist=False,color='blue',label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gDLigxpnfgeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Better results. "
      ],
      "metadata": {
        "id": "IJSFnZ5QgUmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "file = open('laptoppricepredictor.pkl','wb')\n",
        "pickle.dump(rf1,file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "KtPjrJNRgR_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}